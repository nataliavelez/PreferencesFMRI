---
title: "Behavioral pilot analysis"
author: "Natalia VÃ©lez, Yuan Chang Leong"
date: "7/9/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

Load libraries:
```{r warning=FALSE, echo=FALSE, message=FALSE}
library(R.matlab)
library(jsonlite)
library(xlsx)
library(tidyverse)
library(broom)
library(ggthemes)
library(viridis)
library(lme4)
```

List data files:
```{r}
# Directories
cwd = getwd()
project_dir = dirname(cwd)
prescreen_dir = file.path(project_dir, 'prescreening', 'data')
session_dir = file.path(project_dir, 'inlab', 'data_session')
sim_dir = file.path(project_dir, 'inlab', 'sim_data')

# Prescreening data
prescreen_files = list.files(path = prescreen_dir, pattern = '*.json')
prescreen_files = sort(prescreen_files)

# Session data
self_files = list.files(path = session_dir, pattern = 'self')
self_files = sort(self_files)

other_files = list.files(path = session_dir, pattern = 'train|test')
other_files = sort(other_files)

# Simulation files
sim_files = list.files(path = sim_dir, pattern = '*.mat')
sim_files = sort(sim_files)

# IDs to exclude (debugging)
exclude_IDs = c("anon", "SLL_OPUSpilot_00")
```

### Prepare stimuli & simulation outputs

Load movie information:
```{r}
movie_file = file.path(project_dir, 'inlab', 'presentation_scripts', 'movie_info.mat')
movie_data = readMat(movie_file) %>%
  .$movies %>%
  drop() %>%
  t() %>%
  as.data.frame() %>%
  select(poster, valence, setting, genre) %>%
  rename(movie = poster) %>%
  mutate(movie = str_extract(movie, '^[a-z0-9]+'),
         valence = ifelse(valence == 'positive', 1, -1),
         setting = ifelse(setting == 'historical', 1, -1),
         genre = ifelse(genre == 'romance', 1, -1))

movie_features = movie_data %>%
  gather(feature, true_value, valence:genre) %>%
  mutate(feature = factor(feature, levels = c('valence', 'setting', 'genre'))) %>%
  arrange(movie, feature)
```

Load model-estimated preferences:
```{r}
model_prefs = NULL

for (f in 1:length(sim_files)) {
  tmp_data = readMat(file.path(sim_dir, sim_files[f]))$sim.agent[,,1]
  tmp_id = str_extract(sim_files[f], 'SLL_OPUSpilot_[0-9]+')
  tmp_sim = data.frame(subject = tmp_id, 
                       consistency = tmp_data$pct.correct,
                       self = t(tmp_data$orig.weights),
                       target = tmp_data$sim.weights) %>%
    rename(self_valence = self.1,
           self_setting = self.2,
           self_genre = self.3,
           target_valence = target.1,
           target_setting = target.2,
           target_genre = target.3)
  
  model_prefs = plyr::rbind.fill(model_prefs, tmp_sim)
}
```

### Prepare behavioral data

Load prescreening  data:
```{r}
feature_ratings = NULL
prescreen_self = NULL

for (f in 1:length(prescreen_files)) {

  tmp_file  = file.path(prescreen_dir, prescreen_files[f])
  tmp_data = fromJSON(tmp_file)
  tmp_id = tmp_data$ID
  
  if (!(tmp_id %in% exclude_IDs)) {
    # Feature ratings
    tmp_feature = tmp_data$meet_trials %>%
      mutate(subject = tmp_id,
             movie = str_extract(movie, '^[a-z0-9]+')) %>%
      select(c(subject, everything()))
    
    feature_ratings = plyr::rbind.fill(feature_ratings,
                                         tmp_feature)
    
    # Choice trials
    tmp_choice = tmp_data$choice_trials %>%
      select(options, key, rt_in_seconds) %>%
      mutate(subject = tmp_id,
             task = 'prescreen',
             run = 1,
             trial = 1:n()) %>%
      unnest(options) %>%
      mutate(options = str_extract(options, '^[a-z0-9]+')) %>%
      group_by(trial) %>%
      mutate(movie = c('movie_a', 'movie_b')) %>%
      ungroup() %>%
      spread(movie, options) %>%
      arrange(trial) %>%
      mutate(choice = ifelse(key == 0, movie_a, movie_b),
             movie_a = factor(movie_a, levels = movie_data$movie),
             movie_a = as.numeric(movie_a),
             movie_b = factor(movie_b, levels = movie_data$movie),
             movie_b = as.numeric(movie_b),
             choice = factor(choice, levels = movie_data$movie),
             choice = as.numeric(choice)) %>%
      select(subject, task, run, trial, movie_a, movie_b, choice, rt_in_seconds)
    
    prescreen_self = plyr::rbind.fill(prescreen_self,
                                      tmp_choice)
  
  }
}

feature_ratings = feature_ratings %>%
  gather(feature, rating, valence:genre) %>%
  mutate(feature = factor(feature, levels = c('valence', 'setting', 'genre'))) %>%
  arrange(subject, movie, feature) %>%
  left_join(movie_features, by = c('movie', 'feature'))
```

Load in-lab self data:
```{r}
session_self = NULL

for (f in 1:length(self_files)) {
  tmp_file = file.path(session_dir, self_files[f])
  
  # Extract information from filename
  tmp_id = str_extract(tmp_file, 'SLL_OPUSpilot_[0-9]+')
  tmp_run = str_extract(tmp_file, '(?<=self\\.)[0-9]')
  tmp_run = as.numeric(tmp_run)
  
  # Lod data
  tmp_data = readMat(tmp_file)$Data[,,1]
  tmp_options = data.frame(tmp_data$options)
  colnames(tmp_options) = c('movie_a', 'movie_b')
  
  if (!(tmp_id %in% exclude_IDs)) {
  
    tmp_choice = data.frame(subject = tmp_id,
                            task = 'in-lab',
                            run = tmp_run,
                            trial = 1:28,
                            choice = as.numeric(tmp_data$subject.choice),
                            rt_in_seconds = as.numeric(tmp_data$rt)) %>%
      cbind(tmp_options) %>%
      mutate(choice = ifelse(choice == 1, movie_a, movie_b)) %>%
      select(subject, task, run, trial, movie_a, movie_b, choice, rt_in_seconds)
    
    session_self = plyr::rbind.fill(session_self, tmp_choice)
  }
}
```

Combine self data:
```{r}
self_choice = rbind(prescreen_self, session_self)
```


Load in-lab other data:
```{r}
other_choice = NULL

for (f in 1:length(other_files)) {
  tmp_file = file.path(session_dir, other_files[f])
  
  # Extract information from filename
  tmp_id = str_extract(tmp_file, 'SLL_OPUSpilot_[0-9]+')
  tmp_task = str_extract(tmp_file, 'train|test')
  tmp_run = str_extract(tmp_file, '(?<=(train|test)\\.)[0-9]')
  tmp_run = as.numeric(tmp_run)
  
  # Load data
  tmp_data = readMat(tmp_file)$Data[,,1]
  tmp_options = data.frame(tmp_data$options)
  colnames(tmp_options) = c('movie_a', 'movie_b')
  
  # Clean up choices
  if (!(tmp_id %in% exclude_IDs)) {
    
    tmp_choice = data.frame(subject = tmp_id,
                            task = tmp_task,
                            run = tmp_run,
                            trial = 1:28,
                            prediction = as.numeric(tmp_data$subject.choice),
                            correct_answer = as.numeric(tmp_data$target.choice),
                            rt_in_seconds = as.numeric(tmp_data$rt)) %>%
      cbind(tmp_options) %>%
      mutate(prediction = ifelse(prediction == 1, movie_a, movie_b),
             correct_answer = ifelse(correct_answer == 1, movie_a, movie_b),
             correct = prediction == correct_answer) %>%
      select(c(subject:trial, movie_a, movie_b, run, trial, correct_answer, prediction, correct, rt_in_seconds))
    
    other_choice = plyr::rbind.fill(other_choice,
                                    tmp_choice)
    
  }
}

# Clean up factors
# other_choice = other_choice %>%
#   mutate(task = factor(task, levels = c('train', 'test'), labels = c('Train', 'Test')),
#          movie_a = factor(movie_a, levels = 1:32, labels = movie_data$movie),
#          movie_b = factor(movie_b, levels = 1:32, labels = movie_data$movie),
#          correct_answer = factor(correct_answer, levels = 1:32, labels = movie_data$movie),
#          prediction = factor(prediction, levels = 1:32, labels = movie_data$movie)) %>%
#   arrange(task)

other_choice = other_choice %>%
  mutate(task = factor(task, levels = c('train', 'test'), labels = c('Train', 'Test'))) %>%
  arrange(task)
```

## Demographic information

```{r}
demographic_data = read.xlsx('demographic_data.xlsx', sheetIndex = 1) %>%
  na.omit()

demographic_data
```

We tested `r nrow(demographic_data)` participants (`r length(which(demographic_data$gender == 'F'))` F, M(SD) age = `r mean(demographic_data$age)`(`r sd(demographic_data$age)`)).

## Behavioral results

### Data quality checks

1. Checking counterbalancing
```{r}
```

2. Checking timing
```{r}
```

3. How consistent are the target's own choices?
`

4. How different were the participant's and the target's choices?
```{r}
human_comparison = self_choice %>%
  rename(self = choice) %>%
  rowwise() %>%
  mutate(movie1 = min(c(movie_a, movie_b)),
         movie2 = max(c(movie_a, movie_b)),
         set = ifelse(task == 'prescreen', 'Train', 'Test')) %>%
  select(subject, set, movie1, movie2, self) %>%
  ungroup()

target_comparison = other_choice %>%
  rename(target = correct_answer, set = task) %>%
  rowwise() %>%
  mutate(movie1 = min(c(movie_a, movie_b)),
         movie2 = max(c(movie_a, movie_b))) %>%
  select(subject, set, movie1, movie2, target) %>%
  ungroup()

human_vs_target = human_comparison %>%
  left_join(target_comparison, by = c("subject", "set", "movie1", "movie2")) %>%
  mutate(match = self == target)
```

Individual participants' match scores:
```{r}
human_vs_target_summ = human_vs_target %>%
  group_by(subject, set) %>%
  summarise(n_trials = n(),
            n_match = length(which(match))) %>%
  ungroup() %>%
  mutate(p_match = n_match/n_trials*100,
         set = factor(set, levels = c('Train', 'Test')))
```

Average match scores, by movie set (i.e., phase):
```{r}
ggplot(human_vs_target_summ, aes(x = set, y = p_match)) +
  geom_hline(yintercept = 50, linetype = 'dotted') +
  geom_boxplot() +
  geom_jitter(width = 0.25) +
  theme_few(base_size = 18) +
  xlab('Stimulus set') +
  ylab('% overlap between self and target')
```

Is there a relationship between how consistent/predictable participants are in their own choices and how much they overlap with the advisor?
```{r}
human_consistency = model_prefs %>%
  select(c(subject, consistency))

model_overlap = human_vs_target %>%
  filter(set == 'Test') %>%
  group_by(subject) %>%
  summarise(n_trials = n(),
            n_match = length(which(match))) %>%
  ungroup() %>%
  mutate(overlap = n_match/n_trials*100) %>%
  select(subject, overlap)

consistency_vs_overlap = human_consistency %>%
  left_join(model_overlap, by = 'subject')

ggplot(consistency_vs_overlap, aes(x = consistency, y = overlap)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  theme_few(base_size = 18)
```

How much do participant's choices in the prescreening overlap with their choices during the training phase?
```{r}
other_overlap = other_choice %>%
  rowwise() %>%
  mutate(movie1 = min(movie_a, movie_b),
         movie2 = max(movie_a, movie_b)) %>%
  ungroup() %>%
  select(subject, task, movie1, movie2, correct_answer, prediction) %>%
  rename(true_target = correct_answer,
         predicted_target = prediction)

self_target_overlap = self_choice %>%
  rowwise() %>%
  mutate(movie1 = min(movie_a, movie_b),
         movie2 = max(movie_a, movie_b)) %>%
  ungroup() %>%
  select(subject, movie1, movie2, choice) %>%
  rename(self = choice) %>%
  left_join(other_overlap, by = c('subject', 'movie1', 'movie2')) %>%
  mutate(overlap = (self == predicted_target)*1,
         correct = (true_target == predicted_target)*1)

self_target_overlap_summ = self_target_overlap %>%
  drop_na(c(self, predicted_target)) %>%
  group_by(subject, task) %>%
  summarise(overlap = mean(overlap),
            correct = mean(correct))
```

```{r}
self_target_overlap_summ %>%
  ungroup() %>%
  do(mean_cl_boot(.$overlap))
```

```{r}
ggplot(self_target_overlap_summ, aes(x=overlap)) +
  geom_histogram(fill = '#42d4f4') +
  geom_vline(xintercept = mean(self_target_overlap_summ$overlap), linetype = 'dashed') +
  facet_grid(. ~ task) +
  xlim(c(0, 1)) +
  theme_few(base_size = 18) +
  xlab('Overlap between choices for self and other (train)') +
  ylab('Count')
```

Is there a relationship between the overlap between prescreening and training choices and training performance?
```{r}
ggplot(self_target_overlap_summ, aes(x = overlap, y = correct)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  facet_grid(. ~ task) +
  theme_few(base_size = 18) +
  xlab('Overlap between choices for self and other') +
  ylab('Performance in training')
```

```{r}
test_overlap = self_target_overlap_summ %>% filter(task == 'Test')
with(test_overlap, cor.test(overlap, correct))
```

### Rate the movies

Do participants' individual feature ratings align with the "true" features of our stimuli?

1. Valence ratings:
```{r}
valence_ratings = feature_ratings %>%
  filter(feature == 'valence') %>%
  mutate(valence = ifelse(true_value == 1, 'Positive', 'Negative'),
         valence = factor(valence, levels = c('Negative', 'Positive'))) %>%
  arrange(true_value, movie, subject)

ggplot(valence_ratings, aes(x = movie, y = rating, fill = valence)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_boxplot() +
  facet_grid(valence ~ ., drop = T, scales = 'free') +
  xlab('Movie') +
  ylab('Rating') +
  ggtitle('Valence') +
  coord_flip() +
  scale_fill_brewer(palette = 'Set1') +
  guides(fill = F) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```

2. Setting:
```{r}
setting_ratings = feature_ratings %>%
  filter(feature == 'setting') %>%
  mutate(setting = ifelse(true_value == 1, 'Historical', 'Futuristic'),
         setting = factor(setting, levels = c('Futuristic', 'Historical'))) %>%
  arrange(true_value, movie, subject)

ggplot(setting_ratings, aes(x = movie, y = rating, fill = setting)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_boxplot() +
  facet_grid(setting ~ ., drop = T, scales = 'free') +
  xlab('Movie') +
  ylab('Rating') +
  ggtitle('Setting') +
  coord_flip() +
  scale_fill_brewer(palette = 'Set2') +
  guides(fill = F) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```

2. Genre:
```{r}
genre_ratings = feature_ratings %>%
  filter(feature == 'genre') %>%
  mutate(genre = ifelse(true_value == 1, 'Romance', 'Action'),
         genre = factor(genre, levels = c('Action', 'Romance'))) %>%
  arrange(true_value, movie, subject)

ggplot(genre_ratings, aes(x = movie, y = rating, fill = genre)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_boxplot() +
  facet_grid(genre ~ ., drop = T, scales = 'free') +
  xlab('Movie') +
  ylab('Rating') +
  ggtitle('Genre') +
  coord_flip() +
  scale_fill_brewer(palette = 'Accent') +
  guides(fill = F) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```

4. Do participants' ratings align more closely to the "true" features for some movies than for others?
```{r}
feature_distance = feature_ratings %>%
  mutate(distance = (rating-true_value)^2) %>%
  group_by(subject, movie) %>%
  summarise(distance = sqrt(sum(distance))) %>%
  arrange(movie, subject)

ggplot(feature_distance, aes(x = movie, y = distance)) +
  geom_boxplot() +
  coord_flip() +
  xlab('Movie') +
  ylab('Euclidean distance between subject ratings and true features')
```

### Self phase

1. How consistent are participant's own choices within the prescreening?

2. ...From the prescreening to the in-lab self test?

### Training phase

1. How well did participants do in the training, overall?
```{r }
train_summ = other_choice %>%
  filter(task == 'Train') %>%
  group_by(subject) %>%
  summarise(n = n(),
            n_correct = length(which(correct)),
            p_correct = n_correct/n) %>%
  rowwise() %>%
  mutate(conf_low = binom.test(n_correct, n)$conf.int[1],
         conf_hi = binom.test(n_correct, n)$conf.int[2],
         binom_p = binom.test(n_correct, n)$p.value,
         sig = ifelse(binom_p < 0.05, '*', ''))

train_summ
```

2. How well did participants do in each block of the training?
```{r}
train_block = other_choice %>%
  filter(task == 'Train') %>%
  mutate(correct01 = as.numeric(correct)) %>%
  group_by(subject, run, trial) %>%
  summarise(accuracy = mean(correct01, na.rm = T)) %>%
  mutate(phase = 'Train',
         abs_trial = 28*(run-1)+trial) %>%
  ungroup() %>%
  select(c(subject, phase, run, abs_trial, accuracy))

ggplot(train_block, aes(x = abs_trial, y = accuracy)) +
  geom_hline(yintercept = .5, linetype = 'dotted') +
  #stat_summary(fun.y = 'mean', geom = 'point', size = 2, alpha = 0.5) +
  stat_summary(fun.y = 'mean', geom = 'line', size = 1, alpha = 0.5) +
  stat_summary(fun.data = 'mean_cl_boot', geom = 'ribbon', size = 1, width = 0, alpha = 0.5) +
  theme_few(base_size = 18) +
  xlab('Trial') +
  ylab('Accuracy') +
  scale_x_continuous(breaks = seq(0, 112, 28)) +
  ggtitle('Accuracy during training')
  guides(color = F)
```

3. Were participants biased by their own preferences?

### Testing phase

1. How well did participants do in the testing phase, overall? 
```{r}
test_summ = other_choice %>%
  filter(task == 'Test') %>%
  group_by(subject) %>%
  summarise(n = n(),
            n_correct = length(which(correct)),
            p_correct = n_correct/n) %>%
  rowwise() %>%
  mutate(conf_low = binom.test(n_correct, n)$conf.int[1],
         conf_hi = binom.test(n_correct, n)$conf.int[2],
         binom_p = binom.test(n_correct, n)$p.value,
         sig = ifelse(binom_p < 0.05, '*', ''))

test_summ
```

```{r}
test_summ %>% 
  ungroup() %>%
  summarise(mean_accuracy = mean(p_correct),
            se_accuracy = sd(p_correct)/sqrt(n()))
```

2. How well did participants do in each block of the testing phase?
```{r}
test_block = other_choice %>%
  filter(task == 'Test') %>%
  group_by(subject, trial) %>%
  summarise(n = n(),
            n_correct = length(which(correct)),
            accuracy = n_correct/n) %>%
  mutate(phase = 'Test') %>%
  select(c(subject, phase, trial, accuracy))

ggplot(test_block, aes(x = trial, y = accuracy)) +
  geom_hline(yintercept = .5, linetype = 'dotted') +
  stat_summary(fun.y = 'mean', geom = 'point', size = 2, alpha = 0.5) +
  stat_summary(fun.y = 'mean', geom = 'line', size = 1, alpha = 0.5) +
  stat_summary(fun.data = 'mean_cl_boot', geom = 'errorbar', size = 1, width = 0, alpha = 0.5) +
  theme_few(base_size = 18) +
  xlab('Run') +
  ylab('Accuracy') +
  ggtitle('Accuracy at test')
  guides(color = F)
```

Plotting train + test together:
```{r}
performance_by_block = rbind(train_block, test_block) %>%
  mutate(phase = factor(phase, levels = c('Train', 'Test')))

ggplot(performance_by_block, aes(x = run, y = accuracy)) +
  geom_hline(yintercept = .5, linetype = 'dotted') +
  stat_summary(fun.y = 'mean', geom = 'point', size = 2) +
  stat_summary(fun.y = 'mean', geom = 'line', size = 1) +
  stat_summary(fun.data = 'mean_cl_boot', geom = 'errorbar', size = 1, width = 0) +
  facet_grid(. ~ phase) +
  theme_few(base_size = 18) +
  xlab('Run') +
  ylab('Accuracy') +
  guides(color = F)
```

3. Is there a relationship between the consistency in participants' own choices and in their performance at test?
```{r}
consistency_vs_accuracy = test_summ %>%
  left_join(model_prefs, by = 'subject') %>%
  select(subject, consistency, p_correct)

with(consistency_vs_accuracy, cor.test(consistency, p_correct))

ggplot(consistency_vs_accuracy, aes(x = consistency, y = p_correct)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  theme_few(base_size = 18) +
  xlab('Consistency in self-choices') +
  ylab('Performance at other-test')
```